<!DOCTYPE html>
<!-- modified from url=(0049)http://cs.stanford.edu/people/karpathy/deepvideo/ -->
<!-- modified from url=https://portrait-nerf.github.io -->
<html class="gr__ee_nthu_edu" lang="en">

<head>
  <!-- Yandex.Metrika counter -->
  <script type="text/javascript" >
    (function(m,e,t,r,i,k,a){m[i]=m[i]||function(){(m[i].a=m[i].a||[]).push(arguments)};
    m[i].l=1*new Date();k=e.createElement(t),a=e.getElementsByTagName(t)[0],k.async=1,k.src=r,a.parentNode.insertBefore(k,a)})
    (window, document, "script", "https://mc.yandex.ru/metrika/tag.js", "ym");

    ym(85587430, "init", {
        clickmap:true,
        trackLinks:true,
        accurateTrackBounce:true
    });
  </script>
  <noscript><div><img src="https://mc.yandex.ru/watch/85587430" style="position:absolute; left:-9999px;" alt="" /></div></noscript>
  <!-- /Yandex.Metrika counter -->

  <!-- CSS includes -->
  <link href="./asset/bootstrap.min.css" rel="stylesheet">
  <link href="./asset/css" rel="stylesheet" type="text/css">
  <link href="./asset/mystyle.css" rel="stylesheet">
  <link href="./Ckin-Video-Player/dist/css/ckin.css" rel="stylesheet"/>
  <link href="./Ckin-Video-Player/demoDist/demo.css" rel="stylesheet"/>
  <link href="./asset/slideshow.css" rel="stylesheet"/>
</head>

<body data-gr-c-s-loaded="true">


<!-- Title -->
<div class="container-fluid" id="header">
  <div class="row">
    <h1>Self-supervised Depth Denoising <br>Using Lower- and Higher-quality RGB-D sensors</h1>
    <div class="authors">
        <a href="mailto: shabanov.ae@phystech.edu" target="_blank">Akhmedkhan Shabanov</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;
        <a href="https://www.linkedin.com/in/ilya-krotov/" target="_blank">Ilya Krotov</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;
        <a href="mailto: chinaev.n@gmail.com" target="_blank">Nikolay Chinaev</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;
        <a href="mailto: poletaev.va@gmail.com" target="_blank">Vsevolod Poletaev</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;
        <a href="mailto: newkozlukov@gmail.com" target="_blank">Sergei Kozlukov</a><sup>3</sup>&nbsp;&nbsp;&nbsp;&nbsp;
        <br>
        <a href="mailto: pasechnik.ig@gmail.com" target="_blank">Igor Pasechnik</a><sup>2</sup>&nbsp;&nbsp;&nbsp;&nbsp;
        <a href="mailto: yakupov.bulat@gmail.com" target="_blank">Bulat Yakupov</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;
        <a href="https://gdude.de/" target="_blank">Artsiom Sanakoyeu</a><sup>4</sup>&nbsp;&nbsp;&nbsp;&nbsp;
        <a href="mailto: vadim.v.lebedev@gmail.com" target="_blank">Vadim Lebedev</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;
        <a href="https://dmitryulyanov.github.io/about" target="_blank">Dmitry Ulyanov</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;
      <br><br>

      <p style="text-align:center;">
        <sup>1</sup>In3D
        &emsp;
        <sup>2</sup>Samsung AI Center, Moscow
        &emsp;
        <sup>3</sup>Skoltech
        &emsp;
        <sup>4</sup>Heidelberg University
      </p>
    </div>
  </div>
</div>

<!-- Teaser -->
<div class="container" id="example">
  <div class="col text-center"  style="margin-top: 0px">
    <img src="./media/teaser_v2.png" width="55%"></img><br>
    <span style="font-size:17px">We propose a new method for depth denoising. Our model learned in a 
      self-supervised way takes color (a) and depth (b) data coming from the sensor of 
      an iPhone X as input and produces a denoised and refined depth (c). For reference, Kinect v2 depth map capture is included (d).
  </div>
</div>

<!--Abstract-->
<div class="container" id="abstract">
  <h2>Abstract</h2>
  Consumer-level depth cameras and depth sensors embedded in mobile devices enable numerous applications, 
        such as AR games and face identification. However, the quality of the captured depth is sometimes insufficient for
         3D reconstruction, tracking and other computer vision tasks. 
      In this paper, we propose a self-supervised depth denoising approach to denoise and
       refine depth coming from a low quality sensor.
      We record simultaneous RGB-D sequences with unzynchronized lower- and higher-quality 
      cameras and solve a challenging problem of aligning sequences both temporally and spatially. 
      We then learn a deep neural network to denoise the lower-quality depth using the matched higher-quality 
      data as a source of supervision signal. 
      We experimentally validate our method against state-of-the-art filtering-based and deep denoising 
      techniques and show its application for 3D object reconstruction tasks where our approach leads to 
      more detailed fused surfaces and better tracking.
</div>

<!--Main Idea-->
<div class="container" id="main_idea">
  <h2>Main Idea</h2>
    We propose learning-based method for depth denoising of lower-quality (LQ) depth sensor's output using supervision of a higher-quality (HQ) depth sensor.  
    We recorded simultaneous RGB-D sequences with unzynchronized lower- and higher-quality cameras. 
    We consider an in-the-wild scenario where the hardware clock synchronization and prior extrinsic calibration for the sensors is not possible.
    The created dataset consists of people captured in a variety of poses and lightning conditions using the rig.
    </p>  

    <p>
      We solve a challenging problem of aligning sequences both temporally and spatially:
      <br>
      <u>Temporal Alignment</u>: for each pair of sequences we seek for a shift that would align the timestamps of the two sensors so that a simple nearest neighbour search between the timestamps give us the best mapping. 
      The correctness of alignment is measured based on spatial alignment score. 
      <br>
      <u>Spacial Alignment</u>: for each pair of matched frames we use Superpoint detector to extract a set of 2D correspondences. We then optimize an extrinsic matrix that would transform HQ sensor coordinate system into the LQ one.
    </p>

    <p>
      To exploit temporal information available in the consecutive frames we use a recurrent model. We utilize two-level training approach based on out-of-fold predictions method. 
      First, we train first-level model to denoise depth on per-frame basis. 
      As a second-level model we train a convolutional recurrent model to account for temporal correlations in the data.
    </p>
</div>

<!-- Video -->
<center>
  <div class="row section" style="margin-bottom: 20px"></div>
  <iframe width="760" height="515" src="https://www.youtube.com/embed/HD6VdwO9s70" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center>

<!-- Paper -->
<div class="container" id="paper">
  <h2>Paper</h2>
  <div class="col-sm-12">
    <a href="https://arxiv.org/abs/2009.04776" target="_blank">
      <p style="text-align:center;">
        <img height="100" src="./pics/pdf.svg"><br>
        arXiv</p>
    </a>
  </div>
</div>

<br>

  <!-- BibTex -->
  <div class="container" id="main_idea">
  <h4><b>BibTex</b></h4>
  <div>
  <pre class="citation">@article{Ahan-LQ2HQ,
    Author = {Akhmedkhan Shabanov and Ilya Krotov and
                 Nikolay Chinaev and Vsevolod Poletaev and
                 Sergei Kozlukov and Igor Pasechnik and
                 Bulat Yakupov and Artsiom Sanakoyeu and
                 Vadim Lebedev and Dmitry Ulyanov},
    title = {Self-supervised Depth Denoising Using Lower- and Higher-quality RGB-D sensors},
    booktitle={8th International Conference on 3D Vision},
    url   = {https://arxiv.org/abs/2009.04776},
    year  = {2020},
  }</pre>
  </div>
</div>


<!-- Copyright -->
<!-- <div id="footer">
  <p style="text-align:center;">Copyright Â© Akhmedkhan Shabanov 2021</p>
</div> -->

</body>
</html>
