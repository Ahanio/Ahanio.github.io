<style type="text/css">
 * { padding: 0; margin: 0;}

    .container {
        padding: 0px;
        margin: 0px;
        color: #404040;
        font-size: 11pt;
        font-family: 'Open Sans', sans-serif;
        font-style: normal;
        font-weight: 400;
        color: #444;
        /*background-color: #FFFDFA;*/
        /*background: linear-gradient(to right, white, #eee, white);
        background: -webkit-linear-gradient( right, white, #eee, white)*/
        /*background-image: url("/PaintSwatches_1400x900.jpg");*/
        /*font-family: 'textbook', arial, sans-serif;*/
        /*margin-left: auto;
        margin-right: auto;*/
        /*max-width: 2000px;*/
    }
    /* body {
        background-color: #FFFAFA !important;
        background-color: #FAFAFA !important;
    } */
    body {
        background-color: #ffffff !important;
        background-color: #ffffff !important;
    }

   .section {
      padding-top: 20px;
      padding-bottom: 20px;
    }


   .title {
      color: #000000;
      font-family: 'Roboto Condensed', sans-serif;
      padding-top: 10px;
      padding-bottom: 25px;
      font-size: 30pt;
      line-height: 100%;
      font-weight: 700;
      color: #444;
    }
    
    .author {
      font-size: 13pt;
    }

    .section_title {
      color: #000000;
      font-family: 'Roboto Condensed', sans-serif;
      /*padding-top: 10px;*/
      /*padding-bottom: 10px;*/
      font-size: 24pt;
      line-height: 100%;
      font-weight: 700;
      padding-bottom: 10px;
      color: #444;
      /*margin-left: auto;
        margin-right: auto;
      /*line-height: 100%;*/
      /*font-weight: 700;*/
    }
    
  .shadow {
      -webkit-box-shadow: 5px 5px 5px #aaa;
      -moz-box-shadow: 5px 5px 5px #aaa;
      box-shadow: 5px 5px 5px #aaa;
      margin-bottom: 10px;
  }

  .aff {
      margin-bottom: 16px;
      /*max-width: 40px;*/
      max-height: 30px
  }



    .img_authors {
      position: relative;
      max-width: 130px;
      max-height: 130px
    }

    .img_links {
      position: relative;
      max-width: 70px;
      max-height: 70px
    }


.text {
    position:relative;
    line-height:2em;
    overflow:hidden;
}
.fadingEffect {
    position:absolute;
    top:0; bottom:0; right:0;
    width:100%;
    background:#FFFAFA;
    -moz-animation: showHide 5s ease-in alternate infinite; /* Firefox */
    -webkit-animation: showHide 5s ease-in alternate infinite; /* Safari and Chrome */
    -ms-animation: showHide 5s ease-in alternate infinite; /* IE10 */
    -o-animation: showHide 5s ease-in alternate infinite; /* Opera */
    animation: showHide 5s ease-in alternate infinite;
}
@-webkit-keyframes showHide { /* Chrome, Safari */
    0% {width:100%}
    40% {width:0%}
    60% {width:0%;}
    100% {width:100%;}
}
@-moz-keyframes showHide { /* FF */
    0% {width:100%}
    40% {width:0%}
    60% {width:0%;}
    100% {width:100%;}
}
@-ms-keyframes showHide { /* IE10 */
    0% {width:100%}
    40% {width:0%}
    60% {width:0%;}
    100% {width:100%;}
}
@-o-keyframes showHide { /* Opera */
    0% {width:100%}
    40% {width:0%}
    60% {width:0%;}
    100% {width:100%;}
}
@keyframes showHide {
    0% {width:100%}
    40% {width:0%}
    60% {width:0%;}
    100% {width:100%;}
}


#cf {
  position:relative;
  max-height:301px;
  /*width:450px;*/

  width:100%;
  height:100%;
  /*overflow:hidden; */
  /*width:inherit;*/

  /*max-height: auto;*/

  /*margin:0 auto;*/
}

#cf img {
  position:absolute;
  
  /*height:100%;*/
  /*width:100%;*/

  opacity: 0;
  left:0;
  -webkit-transition: opacity 1s ease-in-out;
  -moz-transition: opacity 1s ease-in-out;
  -o-transition: opacity 1s ease-in-out;
  transition: opacity 1s ease-in-out;
  animation-name: cfFadeInOut;
  animation-timing-function: ease-in-out;
  animation-iteration-count: infinite;
  animation-duration: 28s;  /* 7*4 */
}


@keyframes cfFadeInOut {
  /* fade out */
  0% {
    opacity:0;
  }
  5% {
    opacity:1;
  }
  /* fade in */
  14.28% {
    opacity:1;
  }
  19.28% {
    opacity:0;
  }
}

#cf img:nth-of-type(7) {
  animation-delay: 24s;
}
#cf img:nth-of-type(6) {
  animation-delay: 20s;
}
#cf img:nth-of-type(5) {
  animation-delay: 16s;
}
#cf img:nth-of-type(4) {
  animation-delay: 12s;
}
#cf img:nth-of-type(3) {
  animation-delay: 8s;
}
#cf img:nth-of-type(2) {
  animation-delay: 4s;
}
#cf img:nth-of-type(1) {
  animation-delay: 0s;
}

.row-centered {
    text-align:center;
}
.col-centered {
    display:inline-block;
    float:none !important;
    /* reset the text-align */
    text-align:center;
    /* inline-block space fix */
    margin-right:-4px;
    min-width: 170px;
}

html, body {
  margin-top: 0px;
  margin-bottom: 0px;
  margin-left: 10px;
  margin-right: 10px;
  height: 100%;             /* need for iframe height 100% to work */
}

iframe {
  box-sizing: border-box;   /* make the border size be included in the height */
  /*display: block;            make them block to fix white space margin */
  width: 100%;
}

.hint {
  color: #A00;
  font-size: 10pt;
  margin-bottom: 10px;
}

.img-process {
  /*width: 100%;*/
  margin: 5px;
  max-height: 200px; 
  -webkit-transition: opacity 0.2s ease-in-out;
  -moz-transition: opacity 0.2s ease-in-out;
  -ms-transition: opacity 0.2s ease-in-out;
  -o-transition: opacity 0.2s ease-in-out;
  transition: opacity 0.2s ease-in-out;
  opacity: 1;
}
.img-process:hover{
  cursor: pointer;
  opacity: 0.6;
}

.centeredd {
  position: fixed;
  top: 50%;
  left: 50%;
  transform: translate(-50%, -50%);
  -webkit-transform: translate(-50%, -50%);
  -moz-transform: translate(-50%, -50%);
  -o-transform: translate(-50%, -50%);
  -ms-transform: translate(-50%, -50%);
}

.embed-container {
    position: relative;
    padding-bottom: 56.25%;
    height: 0;
    overflow: hidden;
    max-width: 100%;
    margin-bottom: 1em;
}

.embed-container iframe, .embed-container object, .embed-container embed {
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
}

</style>

<!doctype html>
<html lang="en">
  <head>
      <!-- <meta name="viewport" content="width=device-width, initial-scale=1.0"> -->
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
      <title>Self-supervised Depth Denoising Using Lower- and Higher-quality RGB-D sensors</title>
      <!-- <meta property="og:image" content="https://raw.githubusercontent.com/DmitryUlyanov/deep-image-prior/master/data/teaser_compiled.jpg"/> -->
      <meta property="og:title" content="Self-supervised Depth Denoising Using Lower- and Higher-quality RGB-D sensors" />

      <!-- Latest compiled and minified CSS -->
      <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

      <!-- Optional theme -->
      <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp" crossorigin="anonymous"><!-- <link rel="stylesheet" href="../css/bootstrap-theme.min.css"> -->

      <!-- Google fonts -->
      <!-- <link href="../css/google-fonts.css" rel="stylesheet" type="text/css"> -->
      <!-- <link href="https://fonts.googleapis.com/css?family=Roboto+Condensed:400,700|Open+Sans:300italic,400italic,600italic,400,700,300,600" rel="stylesheet" type="text/css"> -->

      <!-- Photoswipe -->
      <!-- <link href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.css" rel="stylesheet">
      <link href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/default-skin/default-skin.css" rel="stylesheet"> -->

      <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
        TeX: { equationNumbers: { autoNumber: "AMS" } },
      });
      </script>

    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
  <script>
  function resizeIframe(obj) {
    obj.style.height = 0;
    obj.style.height = (obj.contentWindow.document.body.scrollHeight + 5) + 'px';
  }




  
</script>
    <script language="JavaScript">

      function resize_iframe()
      {
        obj = document.getElementById("myiframe")

        obj.style.height = 0;
        obj.style.height = (obj.contentWindow.document.body.scrollHeight) + 'px';
        
        document.getElementById("cf").style.height = document.getElementById("lastimage").height + 8;
      }
      window.onresize=resize_iframe; 
      

    </script>

  </head>

  <body>
    <script src="zepto.min.js"></script>
  <script>
    window.addEventListener('message', function(event) {
      if(height = event.data['height']) {
        $('iframe').css('height', height + 'px')
      }
    });
    

    // var resizeEvent = new Event('resize');
    // window.dispatchEvent(resizeEvent);
    
  </script>
  <div class="container">


    <!-- ====================================================== -->
    <!-- ===================== TITLE ========================== -->
    <!-- ====================================================== -->
    <center>
          <div class="title">Self-supervised Depth Denoising Using Lower- and Higher-quality RGB-D sensors</div>
    </center>
   

 <center>
      <div class="row row-eq-height" style="margin-bottom: -10px">
        <!-- <div class="col-sm-12"> -->
        <div class="col-sm-8 col-sm-offset-2">
          <div class="col-sm-4" style="margin-bottom: 10px">
            <!-- <div class="col-sm-12"> -->
              <a href="shabanov.ae@phystech.edu">
                  <div class="author"><span style="font-weight: 600">Akhmedkhan Shabanov</span></div>
              </a>
            <!-- </div> -->
          </div>

          <div class="col-sm-4" style="margin-bottom: 10px">
            <!-- <div class="col-sm-12"> -->
              <a href="ilya.krotov@phystech.edu">
                  <div class="author"><span style="font-weight: 600">Ilya Krotov</span></div>
              </a>
            <!-- </div> -->
          </div>

          <div class="col-sm-4" style="margin-bottom: 10px">
            <!-- <div class="col-sm-12"> -->
              <a href="chinaev.n@gmail.com">
                  <div class="author"><span style="font-weight: 600">Nikolay Chinaev</span></div>
              </a>
            <!-- </div> -->
          </div>

          <div class="col-sm-4" style="margin-bottom: 10px">
            <!-- <div class="col-sm-12"> -->
              <a href="poletaev.va@gmail.com">
                  <div class="author"><span style="font-weight: 600">Vsevolod Poletaev</span></div>
              </a>
            <!-- </div> -->
          </div>

          <div class="col-sm-4" style="margin-bottom: 10px">
            <!-- <div class="col-sm-12"> -->
              <a href="newkozlukov@gmail.com">
                  <div class="author"><span style="font-weight: 600">Sergei Kozlukov</span></div>
              </a>
            <!-- </div> -->
          </div>

          <div class="col-sm-4" style="margin-bottom: 10px">
            <!-- <div class="col-sm-12"> -->
              <a href="pasechnik.ig@gmail.com">
                  <div class="author"><span style="font-weight: 600">Igor Pasechnik</span></div>
              </a>
            <!-- </div> -->
          </div>

          <div class="col-sm-4" style="margin-bottom: 10px">
            <!-- <div class="col-sm-12"> -->
              <a href="yakupov.bulat@gmail.com">
                  <div class="author"><span style="font-weight: 600">Bulat Yakupov</span></div>
              </a>
            <!-- </div> -->
          </div>

          <div class="col-sm-4" style="margin-bottom: 10px">
            <!-- <div class="col-sm-12"> -->
              <a href="artsiom.sanakoyeu@iwr.uni-heidelberg.de">
                  <div class="author"><span style="font-weight: 600">Artsiom Sanakoyeu</span></div>
              </a>
            <!-- </div> -->
          </div>

          <div class="col-sm-4" style="margin-bottom: 10px">
            <!-- <div class="col-sm-12"> -->
              <a href="vadim.v.lebedev@gmail.com">
                  <div class="author"><span style="font-weight: 600">Vadim Lebedev</span></div>
              </a>
            <!-- </div> -->
          </div>
          

          <div class="col-sm-12" style="margin-bottom: 10px">
            <!-- <div class="col-sm-12"> -->
              <a href="https://dmitryulyanov.github.io/about">
                  <div class="author"><span style="font-weight: 600">Dmitry Ulyanov</span></div>
              </a>
            <!-- </div> -->
          </div>

          <div class="col-sm-12" style="margin-bottom: 10px">
            <!-- <div class="col-sm-12"> -->
              <a href="https://in3d.io">
                  <div class="author"><span style="font-weight: 600; color: #444">In3D</span></div>
              </a>
            <!-- </div> -->
          </div>

        </div>
      <!-- </div> -->
    </div>
    </center>

  <!-- ====================================================== -->
  <!-- ===================== TEASER ============== -->
  <!-- ====================================================== -->

  <br>
  <center>
    <div class="row section center row-eq-height">
      <div class="col center col-sm-15" style="margin-top: 10px">
          <center>
          <img src="./assets/depth_denoising/teaser_v3.png" class="img-responsive"/>
          </center>
        <span style="font-size:17px"><i>We propose a new method for depth denoising. Our model learned in a 
          self-supervised way takes color (a) and depth (b) data coming from the sensor of 
          an iPhone X as input and produces a denoised and refined depth (c). For reference, Kinect v2 depth map capture is included (d)</i>.
 
        </span>
      </div>
    </div>
  </center>



      <!-- ====================================================== -->
    <!-- ===================== LINKS ========================== -->
    <!-- ====================================================== -->
    <center>
      <div class="row section" style="margin-bottom: -12px">
        <div class="col-sm-10 col-sm-offset-1">
          <div class="col-sm-12 center">
            <div class="col-sm-12">
              <a href="https://arxiv.org/abs/2009.04776">
                  <img src="./assets/projects/pdf.svg" class="img_links img-responsive">
                  <div class="author">Paper</div>
              </a>
            </div>

        </div>
      </div>
    </div>
    </center>


    <!-- ====================================================== -->
    <!-- ===================== ABSTRACT ======================= -->
    <!-- ====================================================== -->
    <div class="row section">
      <!-- <div class="col-xs-8 col-xs-offset-2 text-justify"> -->
      <div class="section_title">Abstract</div>
      <p> <!-- style="text-indent: 30px" -->
        Consumer-level depth cameras and depth sensors embedded in mobile devices enable numerous applications, 
        such as AR games and face identification. However, the quality of the captured depth is sometimes insufficient for
         3D reconstruction, tracking and other computer vision tasks. 
      In this paper, we propose a self-supervised depth denoising approach to denoise and
       refine depth coming from a low quality sensor.
      We record simultaneous RGB-D sequences with unzynchronized lower- and higher-quality 
      cameras and solve a challenging problem of aligning sequences both temporally and spatially. 
      We then learn a deep neural network to denoise the lower-quality depth using the matched higher-quality 
      data as a source of supervision signal. 
      We experimentally validate our method against state-of-the-art filtering-based and deep denoising 
      techniques and show its application for 3D object reconstruction tasks where our approach leads to 
      more detailed fused surfaces and better tracking.
      </p>
      <!-- </div> -->
    </div>

    <!-- ====================================================== -->
    <!-- ===================== Main idea ======================= -->
    <!-- ====================================================== -->
    <div class="row section">
      <div class="section_title">Main Idea</div>

      <center>
        <div class="row section center row-eq-height">
          <div class="col-sm-10 col-sm-offset-1">
          <div class="col center col-sm-12" style="margin-top: 10px">
              <center>
              <img src="./assets/depth_denoising/poster.jpeg" class="img-responsive"/>
              </center>
              <!-- <div class="author"> Method Overview</div> -->
          </div>
        </div>
      </div>
      </center>
      <!-- <p>
      A learning-based method for depth denoising of lower-quality (LQ) depth sensor’s output using supervision of a higher-quality (HQ) depth sensor is proposed.  We recorded simultaneous RGB-D sequences with unzynchronized lower- and higher-quality cameras. The created dataset consists of people captured in a variety of poses and lightning conditions using the shown rig.
      <p>
        We propose a method for temporal and spatial alignment of RGB-D sequences, removing the need in accurate checkerboard calibration and hardware clock synchronization.
        The collected data is used for a learning-based method for depth denoising of lower-quality (LQ) depth sensor  using supervision of a higher-quality (HQ) depth sensor. While the proposed method can be used with any pair of RGB-D sensors, in this work, we aim to denoise lower-quality Apple TrueDepth (TD) camera and use Microsoft Kinect V2 (K2) as supervision.
      </p>
      To exploit temporal information available in the consecutive frames we to use a recurrent model. We utilize two-level training approach based on out-of-fold predictions method. First, we train first-level model (<i>ours(basic)</i>) to denoise depth on per-frame basis. As a second-level model we train a convolutional recurrent model (<i>ours(full)</i>) to account for temporal correlations in the data.  -->

      <p>
      We propose learning-based method for depth denoising of lower-quality (LQ) depth sensor’s output using supervision of a higher-quality (HQ) depth sensor.  
      We recorded simultaneous RGB-D sequences with unzynchronized lower- and higher-quality cameras. 
      We consider an in-the-wild scenario where the hardware clock synchronization and prior extrinsic calibration for the sensors is not possible.
      The created dataset consists of people captured in a variety of poses and lightning conditions using the rig.
      </p>  

      <p>
        We solve a challenging problem of aligning sequences both temporally and spatially:

        <br>
        <u>Temporal Alignment</u>: for each pair of sequences we seek for a shift ∆H→L that would align the timestamps of the two sensors so that a simple nearest neighbour search between the timestamps give us the best mapping. 
        The correctness of alignment is measured based on spatial alignment score. 
        <br>
        <u>Spacial Alignment</u>: for each pair of matched frames we use Superpoint detector to extract a set of 2D correspondences. We then optimize an extrinsic matrix TH→L that would transform HQ sensor coordinate system into the LQ one.
      </p>

      <p>
        To exploit temporal information available in the consecutive frames we to use a recurrent model. We utilize two-level training approach based on out-of-fold predictions method. First, we train first-level model (<i>ours(basic)</i>) to denoise depth on per-frame basis. As a second-level model we train a convolutional recurrent model (<i>ours(full)</i>) to account for temporal correlations in the data.  -->
      </p>
    </div>


  <div class="row section">
    <div class="section_title">Video</div>
  </div>
    <center>
      <div class="row section" style="margin-bottom: -12px">
        <div class="col-sm-13 col-sm-offset-0">
          <div class="container-fluid">
            <div class="col-sm-13">
              <div class="embed-container">
                <iframe src="https://www.youtube.com/embed/HD6VdwO9s70" frameborder="0" allowfullscreen></iframe>
              </div>
              <div class="author"> Presentation at 3DV2020</div>
            </div>
          </div>
      </div>
    </div>
    </center>



    <!-- <iframe width="560" height="315" src="https://www.youtube.com/embed/mz5Ys2Zeb6A" 
      frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; 
      gyroscope; picture-in-picture" allowfullscreen>
    </iframe> -->


    <!-- <div class="container-fluid">
      <div class="col-sm-7">
        <div class="embed-container">
          <iframe src="https://www.youtube.com/embed/29-tKQzmKJM" frameborder="0" allowfullscreen></iframe>
        </div>
        3DV Presentation Video
      </div>
    </div> -->

    <!-- ====================================================== -->
    <!-- ===================== Main idea ======================= -->
    <!-- ====================================================== -->
    <!-- <div class="row section">
      <div class="section_title">Main idea</div>
      <p>
      We propose a learning-based method for depth denoising of lower-quality (LQ) depth sensor’s output using supervision of a higher-quality (HQ) depth sensor.  We record simultaneous RGB-D sequences with unzynchronized lower- and higher-quality cameras. 
      Secondly, we use the collected data for a learning-based method for depth denoising of lower-quality (LQ) depth sensor  using supervision of a higher-quality (HQ) depth sensor. While the proposed method can be used with any pair of RGB-D sensors, in this work, we aim to denoise lower-quality Apple TrueDepth (TD) camera and use Microsoft Kinect V2 (K2) as supervision.
      </p>
      To exploit temporal information available in the consecutive frames we to use a recurrent model. We utilize two-level training approach based on out-of-fold predictions method. First, we train first-level model (<i>ours(basic)</i>) to denoise depth on per-frame basis. As a second-level model we train a convolutional recurrent model (<i>ours(full)</i>) to account for temporal correlations in the data. 
      <p>

      </p>
    </div> -->

    <!-- ====================================================== -->
    <!-- ============== Comparison ============================ -->
    <!-- ====================================================== -->
    
    <!-- <div class="section_title"><span style="font-size: 18pt">Comparison</span></div>

<div class="container-fluid">
  <div class="col-sm-6">
    <div class="embed-container">
      <iframe src="https://www.youtube.com/embed/29-tKQzmKJM" frameborder="0" allowfullscreen></iframe>
    </div>
  </div>
  <div class="col-sm-6">
    <div class="embed-container">
      <iframe src="https://www.youtube.com/embed/cv8RpeSzVig" frameborder="0" allowfullscreen></iframe>
    </div>
  </div>
</div>

    <div class="container">
    <br>
    <div class="row section" style="padding-bottom: 0px">
      <div class="section_title">Methods compared with</div>
      <div style="font-size: 10pt; margin-left: 35px">
        <ul>
      <li>
        Q. Zhang, X. Shen, L. Xu, and J. Jia, <a href="http://xiaoyongshen.me/papers/rollingfilter.pdf">"Rolling guidance filter"</a> ECCV, 2014
      </li>
      <li> J.  Jeon  and  S.  Lee,  <a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Junho_Jeon_Reconstruction-based_Pairwise_Depth_ECCV_2018_paper.pdf">"Reconstruction-based  pairwise  depth dataset for depth image enhancement using CNN"</a> ECCV, 2018
      </li>
      <li>
        V. Sterzentsenko,  L. Saroglou,  A. Chatzitofis,  S. Thermos,N. Zioulis, A. Doumanoglou, D. Zarpalas, and P. Daras, <a href="https://arxiv.org/abs/1909.01193">"Self-supervised deep depth denoising"</a> ICCV, 2019
      </li>
      <li>S. Yan, C. Wu, L. Wang, F. Xu, L. An, K. Guo, and Y. Liu, <a href="http://www.liuyebin.com/DDRNet/DDRNet.pdf">"DDRNet: Depth map denoising and refinement for consumer depth cameras using cascaded CNNs"</a> ECCV, 2019
      </li>
      <li>
        C. Tomasi and R. Manduchi., <a href="https://users.soe.ucsc.edu/~manduchi/Papers/ICCV98.pdf">"Bilateral filtering for gray and color images"</a> ICCV, 1998
      </li>
       </ul>
    </div>
    </div> -->

</body>
</html>
 
