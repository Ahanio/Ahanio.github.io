<style type="text/css">
 * { padding: 0; margin: 0;}

    .container {
        padding: 0px;
        margin: 0px;
        color: #404040;
        font-size: 11pt;
        font-family: 'Open Sans', sans-serif;
        font-style: normal;
        font-weight: 400;
        color: #444;
        /*background-color: #FFFDFA;*/
        /*background: linear-gradient(to right, white, #eee, white);
        background: -webkit-linear-gradient( right, white, #eee, white)*/
        /*background-image: url("/PaintSwatches_1400x900.jpg");*/
        /*font-family: 'textbook', arial, sans-serif;*/
        /*margin-left: auto;
        margin-right: auto;*/
        /*max-width: 2000px;*/
    }
    /* body {
        background-color: #FFFAFA !important;
        background-color: #FAFAFA !important;
    } */
    body {
        background-color: #ffffff !important;
        background-color: #ffffff !important;
    }

   .section {
      padding-top: 20px;
      padding-bottom: 20px;
    }


   .title {
      color: #000000;
      font-family: 'Roboto Condensed', sans-serif;
      padding-top: 10px;
      padding-bottom: 25px;
      font-size: 30pt;
      line-height: 100%;
      font-weight: 700;
      color: #444;
    }
    
    .author {
      font-size: 13pt;
    }

    .section_title {
      color: #000000;
      font-family: 'Roboto Condensed', sans-serif;
      /*padding-top: 10px;*/
      /*padding-bottom: 10px;*/
      font-size: 24pt;
      line-height: 100%;
      font-weight: 700;
      padding-bottom: 10px;
      color: #444;
      /*margin-left: auto;
        margin-right: auto;
      /*line-height: 100%;*/
      /*font-weight: 700;*/
    }
    
  .shadow {
      -webkit-box-shadow: 5px 5px 5px #aaa;
      -moz-box-shadow: 5px 5px 5px #aaa;
      box-shadow: 5px 5px 5px #aaa;
      margin-bottom: 10px;
  }

  .aff {
      margin-bottom: 16px;
      /*max-width: 40px;*/
      max-height: 30px
  }



    .img_authors {
      position: relative;
      max-width: 130px;
      max-height: 130px
    }

    .img_links {
      position: relative;
      max-width: 70px;
      max-height: 70px
    }


.text {
    position:relative;
    line-height:2em;
    overflow:hidden;
}
.fadingEffect {
    position:absolute;
    top:0; bottom:0; right:0;
    width:100%;
    background:#FFFAFA;
    -moz-animation: showHide 5s ease-in alternate infinite; /* Firefox */
    -webkit-animation: showHide 5s ease-in alternate infinite; /* Safari and Chrome */
    -ms-animation: showHide 5s ease-in alternate infinite; /* IE10 */
    -o-animation: showHide 5s ease-in alternate infinite; /* Opera */
    animation: showHide 5s ease-in alternate infinite;
}
@-webkit-keyframes showHide { /* Chrome, Safari */
    0% {width:100%}
    40% {width:0%}
    60% {width:0%;}
    100% {width:100%;}
}
@-moz-keyframes showHide { /* FF */
    0% {width:100%}
    40% {width:0%}
    60% {width:0%;}
    100% {width:100%;}
}
@-ms-keyframes showHide { /* IE10 */
    0% {width:100%}
    40% {width:0%}
    60% {width:0%;}
    100% {width:100%;}
}
@-o-keyframes showHide { /* Opera */
    0% {width:100%}
    40% {width:0%}
    60% {width:0%;}
    100% {width:100%;}
}
@keyframes showHide {
    0% {width:100%}
    40% {width:0%}
    60% {width:0%;}
    100% {width:100%;}
}


#cf {
  position:relative;
  max-height:301px;
  /*width:450px;*/

  width:100%;
  height:100%;
  /*overflow:hidden; */
  /*width:inherit;*/

  /*max-height: auto;*/

  /*margin:0 auto;*/
}

#cf img {
  position:absolute;
  
  /*height:100%;*/
  /*width:100%;*/

  opacity: 0;
  left:0;
  -webkit-transition: opacity 1s ease-in-out;
  -moz-transition: opacity 1s ease-in-out;
  -o-transition: opacity 1s ease-in-out;
  transition: opacity 1s ease-in-out;
  animation-name: cfFadeInOut;
  animation-timing-function: ease-in-out;
  animation-iteration-count: infinite;
  animation-duration: 28s;  /* 7*4 */
}


@keyframes cfFadeInOut {
  /* fade out */
  0% {
    opacity:0;
  }
  5% {
    opacity:1;
  }
  /* fade in */
  14.28% {
    opacity:1;
  }
  19.28% {
    opacity:0;
  }
}

#cf img:nth-of-type(7) {
  animation-delay: 24s;
}
#cf img:nth-of-type(6) {
  animation-delay: 20s;
}
#cf img:nth-of-type(5) {
  animation-delay: 16s;
}
#cf img:nth-of-type(4) {
  animation-delay: 12s;
}
#cf img:nth-of-type(3) {
  animation-delay: 8s;
}
#cf img:nth-of-type(2) {
  animation-delay: 4s;
}
#cf img:nth-of-type(1) {
  animation-delay: 0s;
}

.row-centered {
    text-align:center;
}
.col-centered {
    display:inline-block;
    float:none !important;
    /* reset the text-align */
    text-align:center;
    /* inline-block space fix */
    margin-right:-4px;
    min-width: 170px;
}

html, body {
  margin-top: 0px;
  margin-bottom: 0px;
  margin-left: 10px;
  margin-right: 10px;
  height: 100%;             /* need for iframe height 100% to work */
}

iframe {
  box-sizing: border-box;   /* make the border size be included in the height */
  /*display: block;            make them block to fix white space margin */
  width: 100%;
}

.hint {
  color: #A00;
  font-size: 10pt;
  margin-bottom: 10px;
}

.img-process {
  /*width: 100%;*/
  margin: 5px;
  max-height: 200px; 
  -webkit-transition: opacity 0.2s ease-in-out;
  -moz-transition: opacity 0.2s ease-in-out;
  -ms-transition: opacity 0.2s ease-in-out;
  -o-transition: opacity 0.2s ease-in-out;
  transition: opacity 0.2s ease-in-out;
  opacity: 1;
}
.img-process:hover{
  cursor: pointer;
  opacity: 0.6;
}

.centeredd {
  position: fixed;
  top: 50%;
  left: 50%;
  transform: translate(-50%, -50%);
  -webkit-transform: translate(-50%, -50%);
  -moz-transform: translate(-50%, -50%);
  -o-transform: translate(-50%, -50%);
  -ms-transform: translate(-50%, -50%);
}

.embed-container {
    position: relative;
    padding-bottom: 56.25%;
    height: 0;
    overflow: hidden;
    max-width: 100%;
    margin-bottom: 1em;
}

.embed-container iframe, .embed-container object, .embed-container embed {
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
}

</style>

<!doctype html>
<html lang="en">
  <head>
      <!-- <meta name="viewport" content="width=device-width, initial-scale=1.0"> -->
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
      <title>Self-supervised Depth Denoising Using Lower- and Higher-quality RGB-D sensors</title>
      <!-- <meta property="og:image" content="https://raw.githubusercontent.com/DmitryUlyanov/deep-image-prior/master/data/teaser_compiled.jpg"/> -->
      <meta property="og:title" content="Self-supervised Depth Denoising Using Lower- and Higher-quality RGB-D sensors" />

      <!-- Latest compiled and minified CSS -->
      <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

      <!-- Optional theme -->
      <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp" crossorigin="anonymous"><!-- <link rel="stylesheet" href="../css/bootstrap-theme.min.css"> -->

      <!-- Google fonts -->
      <!-- <link href="../css/google-fonts.css" rel="stylesheet" type="text/css"> -->
      <!-- <link href="https://fonts.googleapis.com/css?family=Roboto+Condensed:400,700|Open+Sans:300italic,400italic,600italic,400,700,300,600" rel="stylesheet" type="text/css"> -->

      <!-- Photoswipe -->
      <!-- <link href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.css" rel="stylesheet">
      <link href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/default-skin/default-skin.css" rel="stylesheet"> -->

      <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
        TeX: { equationNumbers: { autoNumber: "AMS" } },
      });
      </script>

    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
  <script>
  function resizeIframe(obj) {
    obj.style.height = 0;
    obj.style.height = (obj.contentWindow.document.body.scrollHeight + 5) + 'px';
  }




  
</script>
    <script language="JavaScript">

      function resize_iframe()
      {
        obj = document.getElementById("myiframe")

        obj.style.height = 0;
        obj.style.height = (obj.contentWindow.document.body.scrollHeight) + 'px';
        
        document.getElementById("cf").style.height = document.getElementById("lastimage").height + 8;
      }
      window.onresize=resize_iframe; 
      

    </script>

  </head>

  <body>
    <script src="zepto.min.js"></script>
  <script>
    window.addEventListener('message', function(event) {
      if(height = event.data['height']) {
        $('iframe').css('height', height + 'px')
      }
    });
    

    // var resizeEvent = new Event('resize');
    // window.dispatchEvent(resizeEvent);
    
  </script>
  <div class="container">


    <!-- ====================================================== -->
    <!-- ===================== TITLE ========================== -->
    <!-- ====================================================== -->
    <center>
          <div class="title">Self-supervised Depth Denoising Using Lower- and Higher-quality RGB-D sensors</div>
    </center>
   

 <center>
      <div class="row row-eq-height" style="margin-bottom: -10px">
        <!-- <div class="col-sm-12"> -->
        <div class="col-sm-8 col-sm-offset-2">
          <div class="col-sm-4" style="margin-bottom: 10px">
            <!-- <div class="col-sm-12"> -->
              <a href="shabanov.ae@phystech.edu">
                  <div class="author"><span style="font-weight: 600">Akhmedkhan Shabanov</span></div>
              </a>
            <!-- </div> -->
          </div>

          <div class="col-sm-4" style="margin-bottom: 10px">
            <!-- <div class="col-sm-12"> -->
              <a href="ilya.krotov@phystech.edu">
                  <div class="author"><span style="font-weight: 600">Ilya Krotov</span></div>
              </a>
            <!-- </div> -->
          </div>

          <div class="col-sm-4" style="margin-bottom: 10px">
            <!-- <div class="col-sm-12"> -->
              <a href="chinaev.n@gmail.com">
                  <div class="author"><span style="font-weight: 600">Nikolay Chinaev</span></div>
              </a>
            <!-- </div> -->
          </div>

          <div class="col-sm-4" style="margin-bottom: 10px">
            <!-- <div class="col-sm-12"> -->
              <a href="poletaev.va@gmail.com">
                  <div class="author"><span style="font-weight: 600">Vsevolod Poletaev</span></div>
              </a>
            <!-- </div> -->
          </div>

          <div class="col-sm-4" style="margin-bottom: 10px">
            <!-- <div class="col-sm-12"> -->
              <a href="newkozlukov@gmail.com">
                  <div class="author"><span style="font-weight: 600">Sergei Kozlukov</span></div>
              </a>
            <!-- </div> -->
          </div>

          <div class="col-sm-4" style="margin-bottom: 10px">
            <!-- <div class="col-sm-12"> -->
              <a href="pasechnik.ig@gmail.com">
                  <div class="author"><span style="font-weight: 600">Igor Pasechnik</span></div>
              </a>
            <!-- </div> -->
          </div>

          <div class="col-sm-4" style="margin-bottom: 10px">
            <!-- <div class="col-sm-12"> -->
              <a href="yakupov.bulat@gmail.com">
                  <div class="author"><span style="font-weight: 600">Bulat Yakupov</span></div>
              </a>
            <!-- </div> -->
          </div>

          <div class="col-sm-4" style="margin-bottom: 10px">
            <!-- <div class="col-sm-12"> -->
              <a href="artsiom.sanakoyeu@iwr.uni-heidelberg.de">
                  <div class="author"><span style="font-weight: 600">Artsiom Sanakoyeu</span></div>
              </a>
            <!-- </div> -->
          </div>

          <div class="col-sm-4" style="margin-bottom: 10px">
            <!-- <div class="col-sm-12"> -->
              <a href="vadim.v.lebedev@gmail.com">
                  <div class="author"><span style="font-weight: 600">Vadim Lebedev</span></div>
              </a>
            <!-- </div> -->
          </div>
          

          <div class="col-sm-12" style="margin-bottom: 10px">
            <!-- <div class="col-sm-12"> -->
              <a href="https://dmitryulyanov.github.io/about">
                  <div class="author"><span style="font-weight: 600">Dmitry Ulyanov</span></div>
              </a>
            <!-- </div> -->
          </div>

          <div class="col-sm-12" style="margin-bottom: 10px">
            <!-- <div class="col-sm-12"> -->
              <a href="https://in3d.io">
                  <div class="author"><span style="font-weight: 600; color: #444">In3D</span></div>
              </a>
            <!-- </div> -->
          </div>

        </div>
      <!-- </div> -->
    </div>
    </center>

  <!-- ====================================================== -->
  <!-- ===================== TEASER ============== -->
  <!-- ====================================================== -->

  <br>
  <center>
    <div class="row section center row-eq-height">
      <div class="col center col-sm-15" style="margin-top: 10px">
          <center>
          <img src="./assets/depth_denoising/teaser_v3.png" class="img-responsive"/>
          </center>
        <span style="font-size:17px"><i>We propose a new method for depth denoising. Our model learned in a 
          self-supervised way takes color (a) and depth (b) data coming from the sensor of 
          an iPhone X as input and produces a denoised and refined depth (c). For reference, Kinect v2 depth map capture is included (d)</i>.
 
        </span>
      </div>
    </div>
  </center>





    <!-- ====================================================== -->
    <!-- ===================== ABSTRACT ======================= -->
    <!-- ====================================================== -->
    <div class="row section">
      <!-- <div class="col-xs-8 col-xs-offset-2 text-justify"> -->
      <div class="section_title">Abstract</div>
      <p> <!-- style="text-indent: 30px" -->
        Consumer-level depth cameras and depth sensors embedded in mobile devices enable numerous applications, 
        such as AR games and face identification. However, the quality of the captured depth is sometimes insufficient for
         3D reconstruction, tracking and other computer vision tasks. 
      In this paper, we propose a self-supervised depth denoising approach to denoise and
       refine depth coming from a low quality sensor.
      We record simultaneous RGB-D sequences with unzynchronized lower- and higher-quality 
      cameras and solve a challenging problem of aligning sequences both temporally and spatially. 
      We then learn a deep neural network to denoise the lower-quality depth using the matched higher-quality 
      data as a source of supervision signal. 
      We experimentally validate our method against state-of-the-art filtering-based and deep denoising 
      techniques and show its application for 3D object reconstruction tasks where our approach leads to 
      more detailed fused surfaces and better tracking.
      </p>
      <!-- </div> -->
    </div>
    


    <!-- ====================================================== -->
    <!-- ===================== LINKS ========================== -->
    <!-- ====================================================== -->
    <center>
      <div class="row section" style="margin-bottom: -12px">
        <div class="col-sm-10 col-sm-offset-1">
          <div class="col-sm-12 center">
          <div class="col-sm-5" style="margin-bottom: 12px">
            <div class="col-sm-12">
              <!-- <a href=""> -->
                  <img src="./assets/projects/pdf.svg" class="img_links img-responsive">
                  <div class="author">Paper(Coming Soon)</div>
              <!-- </a> -->
            </div>
          </div>
          <div class="col-sm-7" style="margin-bottom: 12px">
            <div class="col-sm-12">
              <!-- <a href="https://github.com/saic-violet/textured_avatars"> -->
                <img src="./assets/projects/github.svg" class="img_links img-responsive" >
                <div class="author">Code(coming soon)</div>
              <!-- </a> -->
            </div>
          </div>
        </div>
      </div>
    </div>
    </center>


    <!-- ====================================================== -->
    <!-- ===================== Main idea ======================= -->
    <!-- ====================================================== -->
    <div class="row section">
      <div class="section_title">Main idea</div>
        <!-- In image restoration problems the goal is to recover original image $x$ having a corrupted image $x_0$. Such problems are often formulated as an optimization task: 
        
            \begin{equation}\label{eq1}
              \min_x E(x; x_0) + R(x)\,,
            \end{equation}

        where $E(x; x_0)$ is a <i>data term</i> and $R(x)$ is an <i>image prior</i>. The data term $E(x; x_0)$ is usually easy to design for a wide range of problems, such as super-resolution, denoising, inpainting, while image prior $R(x)$ is a challenging one. Today's trend is to capture the prior $R(x)$ with a ConvNet by training it using large number of examples. 
      </p>
      <p>
        We first notice, that for a surjective $g: \theta \mapsto x$ the following procedure in theory is equivalent to \eqref{eq1}:
        
        $$\min_\theta E(g(\theta); x_0) + R(g(\theta)) \,.$$

        In practice $g$ dramatically changes how the image space is searched by an optimization method. Furthermore, by selecting a "good" (possibly injective) mapping $g$, we could get rid of the prior term. We define $g(\theta)$ as $f_\theta(z)$, where $f$ is a deep ConvNet with parameters $\theta$ and $z$ is a fixed input, leading to the formulation

        $$\min_\theta E(f_\theta (z); x_0) \,.$$ 

        Here, the network $f_\theta$ is initialized randomly and input $z$ is filled with noise and fixed. 
      </p>
      <p>
        In other words, <b>instead of searching for the answer in the image space we now search for it in the space of neural network's parameters</b>. We emphasize that we never use a pretrained network or an image database. Only corrupted image $x_0$ is used in the restoration process.  
      </p>
      <p>
        See <a href="https://sites.skoltech.ru/app/data/uploads/sites/25/2018/04/deep_image_prior.pdf">paper</a> and <a href="https://box.skoltech.ru/index.php/s/ib52BOoV58ztuPM">supplementary material</a> for details. 
      </p>
      ---------------- -->
      <p>
      First of all, we propose a pipeline for collecting and aligning RGB-D streams of two different sensors without checkerboard calibration or hardware clock synchronization. 
      Secondly, we use the collected data for a learning-based method for depth denoising of lower-quality (LQ) depth sensor  using supervision of a higher-quality (HQ) depth sensor. While the proposed method can be used with any pair of RGB-D sensors, in this work, we aim to denoise lower-quality Apple TrueDepth (TD) camera and use Microsoft Kinect V2 (K2) as supervision.
      </p>
      To exploit temporal information available in the consecutive frames we to use a recurrent model. We utilize two-level training approach based on out-of-fold predictions method. First, we train first-level model (<i>ours(basic)</i>) to denoise depth on per-frame basis. As a second-level model we train a convolutional recurrent model (<i>ours(full)</i>) to account for temporal correlations in the data. 
      <p>

      </p>
    </div>

    <!-- ====================================================== -->
    <!-- ============== Comparison ============================ -->
    <!-- ====================================================== -->
    
    <div class="section_title">Comparison</div>
    <!-- <iframe id='vid3' width="560" height="700" src="https://www.youtube.com/embed/Q1E1Qkv3XI8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen style="display:none;"></iframe> -->

    <!-- <iframe width="100" height="200"
    src="https://www.youtube.com/embed/Q1E1Qkv3XI8">
    </iframe>
    <iframe width="100" height="200"
    src="https://www.youtube.com/embed/Eg0DreHRSOM">
    </iframe> -->

    <!-- <div style="float:left;" >
      <iframe frameborder="0" id="youtube" src="https://www.youtube.com/embed/Q1E1Qkv3XI8" title="YouTube video player" width="800" height="400" class="graphic"></iframe>
    </div>
    
    <div style="float:right;">
      <iframe frameborder="0" id="youtube" src="https://www.youtube.com/embed/Eg0DreHRSOM" title="YouTube video player" width="800" height="400" class="graphic"></iframe>
    </div>
  </div> -->

<div class="container-fluid">
  <div class="col-sm-6">
    <div class="embed-container">
      <iframe src="https://www.youtube.com/embed/29-tKQzmKJM" frameborder="0" allowfullscreen></iframe>
    </div>
  </div>
  <div class="col-sm-6">
    <div class="embed-container">
      <iframe src="https://www.youtube.com/embed/cv8RpeSzVig" frameborder="0" allowfullscreen></iframe>
    </div>
  </div>
</div>

    <div class="container">
    <br>
    <div class="row section" style="padding-bottom: 0px">
      <div class="section_title">Methods compared with</div>
      <div style="font-size: 10pt; margin-left: 35px">
      <!-- <p> -->
        <ul>
      <li>
        Q. Zhang, X. Shen, L. Xu, and J. Jia, <a href="http://xiaoyongshen.me/papers/rollingfilter.pdf">"Rolling guidance filter"</a> ECCV, 2014
      </li>
      <li> J.  Jeon  and  S.  Lee,  <a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Junho_Jeon_Reconstruction-based_Pairwise_Depth_ECCV_2018_paper.pdf">"Reconstruction-based  pairwise  depth dataset for depth image enhancement using CNN"</a> ECCV, 2018
      </li>
      <li>
        V. Sterzentsenko,  L. Saroglou,  A. Chatzitofis,  S. Thermos,N. Zioulis, A. Doumanoglou, D. Zarpalas, and P. Daras, <a href="https://arxiv.org/abs/1909.01193">"Self-supervised deep depth denoising"</a> ICCV, 2019
      </li>
      <li>S. Yan, C. Wu, L. Wang, F. Xu, L. An, K. Guo, and Y. Liu, <a href="http://www.liuyebin.com/DDRNet/DDRNet.pdf">"DDRNet: Depth map denoising and refinement for consumer depth cameras using cascaded CNNs"</a> ECCV, 2019
      </li>
      <li>
        C. Tomasi and R. Manduchi., <a href="https://users.soe.ucsc.edu/~manduchi/Papers/ICCV98.pdf">"Bilateral filtering for gray and color images"</a> ICCV, 1998
      </li>
       </ul>
    </div>
    </div>

</body>
</html>
 
